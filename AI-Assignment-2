{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Implementando as duas fases da execução:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "from numpy import sqrt, unique\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer, StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RepeatedStratifiedKFold, StratifiedKFold\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Function used to test the methods that don't use hyperparameters:\n",
    "def first_stage(dataset, estimator, use_discretizer = False):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    x = dataset['data']\n",
    "    y = dataset['target']\n",
    "    \n",
    "    rfsk = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3)\n",
    "\n",
    "    score = None\n",
    "    pipeline = None\n",
    "\n",
    "    if use_discretizer: # This is used for the One Rule method\n",
    "        discretizer = KBinsDiscretizer(2 * len(unique(dataset['target'])), encode = 'ordinal', strategy = 'kmeans')\n",
    "        pipeline = Pipeline([('transformer', scaler), ('discretizer', discretizer), ('estimator', estimator)])\n",
    "    else:\n",
    "        pipeline = Pipeline([('transformer', scaler), ('estimator', estimator)])\n",
    "\n",
    "    # Executing the test:\n",
    "    scores = cross_val_score(pipeline, x, y, scoring = 'accuracy', cv = rfsk)\n",
    "\n",
    "    # Getting statistical data:\n",
    "    mean = scores.mean()\n",
    "    deviation = scores.std()\n",
    "    inf, sup = stats.norm.interval(0.95, loc = mean, scale = deviation / sqrt(len(scores)))\n",
    "\n",
    "    return (scores, mean, deviation, inf, sup)\n",
    "\n",
    "# Used for the methods that use hyperparameters:\n",
    "def second_stage(dataset, estimator, param_grid):\n",
    "    x = dataset['data']\n",
    "    y = dataset['target']\n",
    "\n",
    "    inner_cv = StratifiedKFold(n_splits = 4)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    pipeline = Pipeline([('transformer', scaler), ('estimator', estimator)])\n",
    "\n",
    "    # Getting the best hyperparameter configuration:\n",
    "    grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = 'accuracy', cv = inner_cv)\n",
    "    grid_search.fit(x, y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    pipeline = Pipeline([('transformer', scaler), ('clf', grid_search)])\n",
    "\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3)\n",
    "\n",
    "    # Making the test:\n",
    "    scores = cross_val_score(pipeline, x, y, scoring = 'accuracy', cv = outer_cv)\n",
    "\n",
    "    # Getting statistical data:\n",
    "    mean = scores.mean()\n",
    "    deviation = scores.std()\n",
    "    inf, sup = stats.norm.interval(0.95, loc = mean, scale = deviation / sqrt(len(scores)))\n",
    "\n",
    "    return (scores, mean, deviation, inf, sup)"
   ]
  },
  {
   "source": [
    "Para conseguir implementar os classificadores que usam os métodos de agrupamento k-Means e Algoritmo Genético, foi criada uma classe base do método k-Centróides que é extendida pelos métodos específicos citados:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KCentroidsBaseEstimator(BaseEstimator):\n",
    "\n",
    "    def __init__(self, k=1):\n",
    "        self.k = k\n",
    "\n",
    "    # This must be implemented in the classes that extend this one\n",
    "    def clustering(self, points):\n",
    "        return None\n",
    "\n",
    "    # Fitting the k-Centroids Base Estimator\n",
    "    def fit(self, X, Y):\n",
    "        # Getting the points in each class:\n",
    "        self.__classes = np.unique(Y)\n",
    "        points_by_classes = dict({})\n",
    "        for c in self.__classes:\n",
    "            indices = np.where(Y == c)\n",
    "            points_by_classes[c] = np.take(X, indices[0], axis=0)\n",
    "\n",
    "        # Applying the clustering algorithm for each points list and saving the centroids list:\n",
    "        self.__centroids = dict({})\n",
    "        for c, points in points_by_classes.items():\n",
    "            self.__centroids[c] = self.clustering(points)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Prediction is made by getting the class that contains the centroid closest to the point being classified:\n",
    "        Y = []\n",
    "        for point in X:\n",
    "            distances = dict({})\n",
    "            for c in self.__classes:\n",
    "                distances[c] = min([norm(point - centroid) for centroid in self.__centroids[c]])\n",
    "            Y.append(min(distances, key=distances.get))\n",
    "        return Y\n"
   ]
  },
  {
   "source": [
    "Com isso, podemos implementar o classificador k-Means, foi feito o seguinte:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClassifier(KCentroidsBaseEstimator):\n",
    "    \n",
    "    # Implementing the clustering method with a naive k-means algorithm:\n",
    "    def clustering(self, points):\n",
    "        km = KMeans(n_clusters = self.k)\n",
    "        km.fit(points)\n",
    "        return km.cluster_centers_"
   ]
  },
  {
   "source": [
    "Para implementar o classificador com algoritmo genético, foi necessário implementar um método para representar os estados do problema de agrupamento e a implementação da meta-heurística em si. Com isso, temos o seguinte:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the distance between two points:\n",
    "def euclidian_dist(a : np.array, b : np.array) -> float:\n",
    "    result : float = 0\n",
    "    aux : np.array = a - b\n",
    "    aux = aux ** 2\n",
    "    result = np.sum(aux)\n",
    "    return math.sqrt(result)\n",
    "\n",
    "# Class that will hold data about the clusters:\n",
    "class Clusters:\n",
    "    def __init__(self, k : int, data : np.array):\n",
    "        self._k : int = k\n",
    "        self._point_dim : tuple = (1, data.shape[1])\n",
    "        \n",
    "        # Setting up the points array:\n",
    "        self._points_data = np.array(data)\n",
    "        self._num_points : int = self._points_data.shape[0]\n",
    "        np.reshape(self._points_data, (self._num_points, self._point_dim[1]))\n",
    "\n",
    "        # Creating a random seed:\n",
    "        random.seed(time.thread_time_ns())\n",
    "\n",
    "        # Initializing the data arrays:\n",
    "        self._clusters : np.array = np.zeros(self._num_points, dtype=int)\n",
    "        self._centroids = list([])\n",
    "        for i in range(self._k):\n",
    "            self._centroids.append(np.zeros(self._point_dim))\n",
    "        \n",
    "        # Initializing the distubed data arrays:\n",
    "        self._disturbed_clusters : np.array = np.zeros(self._num_points, dtype=int)\n",
    "        self._disturbed_centroids = list([])\n",
    "        for i in range(self._k):\n",
    "            self._disturbed_centroids.append(np.zeros(self._point_dim))\n",
    "        \n",
    "        # Initializing the cost of the regular state and the disturbed state:\n",
    "        self._sse : float = 0\n",
    "        self._disturbed_sse : float = 0\n",
    "\n",
    "        # Initializing the list of sse's for each cluster:\n",
    "        self._sse_per_cluster : list = []\n",
    "        for i in range(self._k):\n",
    "            self._sse_per_cluster.append(0.)\n",
    "\n",
    "    # Initializing the state of the clusters given an array of centroids:\n",
    "    def initialize_state(self, centroids : list = None):\n",
    "        if centroids != None:\n",
    "            self.centroids = centroids\n",
    "        else:\n",
    "            # Calculating the initial values of the centroids:\n",
    "            for i in range(self._k):\n",
    "                index = np.random.choice(self._num_points, size=1, replace=False)[0]\n",
    "                self._centroids[i] = self._points_data[index]\n",
    "\n",
    "        for i in range(self._num_points):\n",
    "            dist : float = np.inf\n",
    "            closest : int = -1\n",
    "            for j in range(self._k):\n",
    "                d : float = euclidian_dist(self._points_data[i], self._centroids[j])\n",
    "                if d < dist:\n",
    "                    dist = d\n",
    "                    closest = j\n",
    "            self._clusters[i] = closest\n",
    "        \n",
    "        for i in range(self._k):\n",
    "            self.calculate_sse_in_cluster(i, self._clusters, self._centroids)\n",
    "        \n",
    "        self._disturbed_sse = 0\n",
    "        self._disturbed_clusters : np.array = np.zeros(self._num_points, dtype=int)\n",
    "        self._disturbed_centroids = list([])\n",
    "        for i in range(self._k):\n",
    "            self._disturbed_centroids.append(np.zeros(self._point_dim))\n",
    "\n",
    "        self._sse = self.calculate_sse()\n",
    "\n",
    "    # Getting the number of clusters:\n",
    "    def get_k(self) -> int:\n",
    "        return self._k\n",
    "    \n",
    "    # Returning the dimension of a single point:\n",
    "    def get_point_dim(self) -> tuple:\n",
    "        return self._point_dim\n",
    "\n",
    "    # Returning the cluster identifier of each point:\n",
    "    def get_clusters(self) -> np.array:\n",
    "        return self._clusters\n",
    "\n",
    "    # Returning the cluster identifier array in the disturbed state:\n",
    "    def get_disturbed_clusters(self) -> np.array:\n",
    "        return self._disturbed_clusters\n",
    "\n",
    "    # Returning the points data in the clusters:\n",
    "    def get_points(self) -> np.array:\n",
    "        return self._points_data\n",
    "    \n",
    "    # Returning the list of centroids of each cluster:\n",
    "    def get_centroids(self) -> list:\n",
    "        return self._centroids\n",
    "    \n",
    "    # Reseting the list of centroids of each cluster:\n",
    "    def set_centroids(self, centroids : list):\n",
    "        for i in range(self._k):\n",
    "            self._centroids[i] = centroids[i]\n",
    "    \n",
    "    # Returning the list of centroids of each cluster in the disturbed state:\n",
    "    def get_disturbed_centroids(self) -> list:\n",
    "        return self._disturbed_centroids\n",
    "    \n",
    "    # Returning the number of points in the set:\n",
    "    def get_num_points(self) -> int:\n",
    "        return self._num_points\n",
    "    \n",
    "    # Getting a single point:\n",
    "    def get_point(self, point_index : int = 0) -> np.array:\n",
    "        return self._points_data[point_index]\n",
    "    \n",
    "    # Getting array of points that are in a given cluster:\n",
    "    def get_points_in_cluster(self, cluster : int, clusters : np.array) -> np.array:\n",
    "        indices : list = list([])\n",
    "        for i in range(self._num_points):\n",
    "            if clusters[i] == cluster:\n",
    "                indices.append(i)\n",
    "        result : np.array = np.take(self.points, indices, axis = 0)\n",
    "        return result\n",
    "    \n",
    "    # Getting the number of points in a given cluster:\n",
    "    def get_num_in_cluster(self, cluster : int, clusters : np.array) -> int:\n",
    "        indices : list = list([])\n",
    "        for i in range(self._num_points):\n",
    "            if clusters[i] == cluster:\n",
    "                indices.append(i)\n",
    "        return len(indices)\n",
    "    \n",
    "    # Changing the cluster of a given point:\n",
    "    def move_point(self, point_index : int = 0, cluster_id : int = 0):\n",
    "        self._clusters[point_index] = cluster_id\n",
    "    \n",
    "    # Calculating the centroid (mean point) of the desired cluster:\n",
    "    def cluster_centroid(self, cluster_id : int, clusters : np.array) -> np.array:\n",
    "        centroid : np.array = np.zeros(self._point_dim[1]) # Initializing the centroid variable\n",
    "\n",
    "        # Getting the points that are inside the target cluster:\n",
    "        points : np.array = self.get_points_in_cluster(cluster_id, clusters)\n",
    "        num_points : int = points.shape[0]\n",
    "\n",
    "        # Calculating the centroid coordinates:\n",
    "        for point in points:\n",
    "            centroid += point / num_points\n",
    "        \n",
    "        return centroid\n",
    "    \n",
    "    # Getting the sum of squares of euclidian distances in the regular state:\n",
    "    def get_sse(self) -> float:\n",
    "        return self._sse\n",
    "    \n",
    "    # Getting the sum of squares of euclidian distances in the disturbed state:\n",
    "    def get_disturbed_sse(self) -> float:\n",
    "        return self._disturbed_sse\n",
    "\n",
    "    # Calculating the sse in a given cluster:\n",
    "    def calculate_sse_in_cluster(self, c : int, clusters : np.array, centroids : list):\n",
    "        points : np.array = self.get_points_in_cluster(c, clusters)\n",
    "        centroids[c] = self.cluster_centroid(c, clusters)\n",
    "        for point in points:\n",
    "            self._sse_per_cluster[c] += euclidian_dist(point, centroids[c]) ** 2\n",
    "\n",
    "    # Calcuting the sum of squares of euclidian distances:\n",
    "    def calculate_sse(self) -> float:\n",
    "        result : float = 0\n",
    "        for c in range(self._k):\n",
    "            result += self._sse_per_cluster[c]\n",
    "        return result\n",
    "\n",
    "    # Getting the point in point_set that is furthest from c\n",
    "    def furthest_in_set(self, point_set : np.array, c : np.array) -> int:\n",
    "        f = lambda p : euclidian_dist(p, c)\n",
    "        dist : float = 0\n",
    "        max_index : int = 0\n",
    "        for i in range(self.get_num_in_cluster(c, self._clusters)):\n",
    "            d : float = f(point_set[i])\n",
    "            if d >= dist:\n",
    "                max_index = i\n",
    "                dist = d\n",
    "        return max_index\n",
    "\n",
    "    def accept_disturbed(self):\n",
    "        self._clusters = copy.deepcopy(self._disturbed_clusters)\n",
    "        self._centroids = copy.deepcopy(self._disturbed_centroids)\n",
    "        self._sse = self._disturbed_sse\n",
    "\n",
    "    # Disturbing the current state to create a possible neighbour:\n",
    "    def disturb(self):\n",
    "        # Disturbance will be achieved by selecting a random cluster and removing the furthest point to the cluster with closest centroid\n",
    "        cluster : int =  random.randint(0, self._k - 1)\n",
    "        points : np.array = self.get_points_in_cluster(cluster, self._clusters)\n",
    "        index : int = self.furthest_in_set(points, cluster)\n",
    "        dist : float = np.inf\n",
    "        closest : int = -1\n",
    "        for c in range(self._k):\n",
    "            d : float = euclidian_dist(self._points_data[index], self._centroids[c])\n",
    "            if d < dist:\n",
    "                dist = d\n",
    "                closest = c\n",
    "        self._disturbed_clusters[index] = closest\n",
    "        self._disturbed_centroids[closest] = self.cluster_centroid(closest, self._disturbed_clusters)\n",
    "        self._disturbed_centroids[cluster] = self.cluster_centroid(cluster, self._disturbed_clusters)\n",
    "        self.calculate_sse_in_cluster(closest, self._disturbed_clusters, self._disturbed_centroids)\n",
    "        self.calculate_sse_in_cluster(cluster, self._disturbed_clusters, self._disturbed_centroids)\n",
    "        self._disturbed_sse = self.calculate_sse()\n",
    "\n",
    "    k = property(get_k)\n",
    "    point_dim = property(get_point_dim)\n",
    "    points = property(get_points)\n",
    "    num_points = property(get_num_points)\n",
    "    sse = property(get_sse)\n",
    "    disturbed_sse = property(get_disturbed_sse)\n",
    "    clusters = property(get_clusters)\n",
    "    disturbed_clusters = property(get_disturbed_clusters)\n",
    "    centroids = property(get_centroids, set_centroids)\n",
    "    disturbed_centroids = property(get_disturbed_centroids)"
   ]
  },
  {
   "source": [
    "Perceba que a perturbação de um estado é feita a partir de uma pequena alteração aleatória num dos centróides, e os pontos são ajustados para o grupo com o centróide mais próximo. Dessa forma, podemos implementar um algoritmo genético para o problema do agrupamento fazendo o cromossomo de cada instância ser a lista de centróides dos grupos do estado, a operação de crossover ser a concatenação de sublistas de duas instâncias, e a operação de mutação ser a operação de perturbação do estado definida acima:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParams:\n",
    "    def __init__(self, population_size : int, crossover_rate : float, mutation_rate : float):\n",
    "        self._population_size = population_size\n",
    "        self._crossover_rate = crossover_rate\n",
    "        self._mutation_rate = mutation_rate\n",
    "    \n",
    "    @property\n",
    "    def population_size(self) -> int:\n",
    "        return self._population_size\n",
    "    \n",
    "    @property\n",
    "    def crossover_rate(self) -> float:\n",
    "        return self._crossover_rate\n",
    "    \n",
    "    @property\n",
    "    def mutation_rate(self) -> float:\n",
    "        return self._mutation_rate\n",
    "\n",
    "class Population:\n",
    "    def __init__(self, k : int, size : int, clusters : clt.Clusters):\n",
    "        self._size = size\n",
    "        self._instances : list = []\n",
    "        self._k : int = k\n",
    "        self._point_dim : tuple = clusters.point_dim\n",
    "        for i in range(size):\n",
    "            self._instances.append(clt.Clusters(clusters.k, clusters.points))\n",
    "            self._instances[i].initialize_state()\n",
    "    \n",
    "    @property\n",
    "    def instances(self) -> list:\n",
    "        return self._instances\n",
    "\n",
    "    def mutate(self, instance : int):\n",
    "        self._instances[instance].disturb()\n",
    "        self._instances[instance].accept_disturbed()\n",
    "    \n",
    "    def crossover(self, a : int, b : int) -> clt.Clusters:\n",
    "        offspring : clt.Clusters = clt.Clusters(self._instances[a].k, self._instances[a].points)\n",
    "        \n",
    "        divide : int = self._k // 2 if self._k <= 3 else random.randint(1, self._k - 2)\n",
    "        centroids : list = list([])\n",
    "        for i in range(self._k):\n",
    "            centroids.append(self._instances[a].centroids[i] if i < divide else self._instances[b].centroids[i])\n",
    "        \n",
    "        offspring.initialize_state(centroids)\n",
    "\n",
    "        return offspring\n",
    "    \n",
    "    def adaptation(self, instance : int) -> float:\n",
    "        sse_sum : float = 0\n",
    "        for i in range(self._size):\n",
    "            sse_sum += self._instances[i].sse\n",
    "        return self._instances[i].sse / sse_sum\n",
    "    \n",
    "    def instance_value(self, instance : int) -> float:\n",
    "        return self._instances[instance].sse\n",
    "    \n",
    "    def coverging(self) -> bool:\n",
    "        s : float = self._instances[0].sse\n",
    "        for i in range(1, self._size):\n",
    "            if abs(s - self._instances[i].sse) > .000000001:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def set_instance(self, instance : int, clusters : clt.Clusters):\n",
    "        self._instances[instance] = clusters\n",
    "\n",
    "    def set_instances(self, instances : list):\n",
    "        for i in range(self._size):\n",
    "            self._instances[i] = instances[i]\n",
    "\n",
    "def genetic(hyper_params : HyperParams, clusters : Clusters, population : Population = None) -> (float, Clusters):\n",
    "\n",
    "    population = Population(clusters.k, hyper_params.population_size, clusters) if population == None else population\n",
    "    \n",
    "    while not population.coverging():\n",
    "\n",
    "        population_indices : list = range(0, hyper_params.population_size)\n",
    "        probabilities : list = list(map(population.adaptation, population_indices))\n",
    "        values : list = list(map(population.instance_value, population_indices))\n",
    "        a : int = random.choices(population_indices, probabilities, k = 1)[0]\n",
    "        b : int = random.choices(population_indices, probabilities, k = 1)[0]\n",
    "        if random.random() < hyper_params.crossover_rate:\n",
    "            offspring : clt.Clusters = population.crossover(a, b)\n",
    "            max_value : float = max(values)\n",
    "            if offspring.sse < max_value:\n",
    "                population.set_instance(values.index(max_value), offspring)\n",
    "    \n",
    "    result : float = min(map(population.instance_value, range(0, hyper_params.population_size)))\n",
    "    result_instace : clt.Clusters = None\n",
    "\n",
    "    for instance in population.instances:\n",
    "        if instance.sse == result:\n",
    "            result_instace = instance\n",
    "\n",
    "    return (result, result_instace)"
   ]
  },
  {
   "source": [
    "Com isso, nós podemos extender a classe do método k-Centróides para implementar a seguinte versão do Classificador k-Genético:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticClassifier(KCentroidsBaseEstimator):\n",
    "    \n",
    "    # Implementing the clustering method with a genetic algorithm:\n",
    "    def clustering(self, points):\n",
    "        params = HyperParams(10, .2, .95)\n",
    "        init_state = Clusters(self.k, points)\n",
    "        _, result = genetic(params, init_state)\n",
    "        return result.centroids"
   ]
  }
 ]
}